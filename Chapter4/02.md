# RAG检索增强生成让你的LLM更聪明
- Query->Embedding->Vector database->Context->Prompt->Generate->Response
    1. 原始文本处理，包括清洗、切片，目的是得到比较纯净的数据，这里一般不太会涉及太多算法。
    2. Ebedding: 将处理后的文本转换为嵌入向量，这一步骤是将文本的语义信息编码为数值形式，使其能够被计算机处理（文本-》空间）。得到的嵌入向量代表了文本的高维数值表示，捕捉了文本中的语义关系和上下文信息。这一步会有很多种算法，比较常见的，比如google的Word2Vec,通过连续词袋和字跳模式，可以生成密集的词向量；Meta开发的FastText,能够处理生僻词和拼写错误的单词；以上两种算法都是基于简单的神经网络架构的（还有很多），而课程里使用的openai embedding则使用了更先进的Transformer架构，这种架构可以生成更高质量和通用的文本嵌入，不过openai没有公布他们的具体算法，但是大概可以推测是通过自注意力机制(self-Attention)机制捕捉序列内各个元素的复杂关系，来将文本转换为嵌入向量的。
    3. 向量数据库：上一步得到了文本的嵌入向量，将这些嵌入向量存储进向量数据库。向量数据库关注如何高效地存储和检索这些高维向量。那么第一步就是构建索引index，以加快后续的检索过程，这里主要涉及近似最新相邻搜索算法ANN，常见的算法包括KD树、FAISS、HNSW等等，比如KD树，就是一种空间划分算法，通过递归地将空间分割成两部分来构建树结构，使得每个节点代表了一个超矩形区域，并以此来优化搜索过程。这里就不展开了，可以自行搜索，他们干的事情都是让向量数据库能够有效地处理和检索大量的高维向量数据。那么有了索引，当有具体问题的时候，就是向量数据库检索过程，这里就不一一展开了，比如最常见的KNN算法(K-Nearest Neighbors)，对于分类任务，当给定一个未标记的观测点时，KNN算法会在数据集中找到与该点最近的K个已标记点，然后根据这K个点的标签通过投票机制决定未标记点的标签。对于回归任务，KNN找到与未知点最近的K个点，然后计算这些点的平均值或加权平均值，作为预测结果。
    4. embedding将文本转为高维向量，向量数据库里实现了向量存储、向量索引、向量检索的过程，当有具体的问题的时候，就会通过向量空间的距离(相似度)，得到一个或一组最相关的文档碎片，这组文档就可以作为LLM回答的参考资料，连同问题一并提交LLM，这样就是一个RAG的基本过程。这中间涉及到很多算法，选择不同的embedding和向量数据库，包括不同的检索方式，都会影响到最终检索的精确度。
    5. 个人感觉很多NLP做习惯的同学，可能接受LLM大力出奇迹的模式需要转个弯，毕竟不通过传统特征工程的做法，LLM的通用性让很多算法其实变的不是很重要，比如embedding的过程，用简单的算法也可以做，用大模型的方式也可以做，无非是准确度、通用性、效率等因素的综合考量（调参变调prompt...）。我们重点在应用开发上，类似向量库底层的东西可以再开一门课了 ：）。希望解答了你的困惑。
- Langchain中RAG的实现
    - Source->Load->Transform->Embed->Store->Retrieve
    - 各种文档->Load->文本切片->嵌入向量化->向量存储->各种检索链